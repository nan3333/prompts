title: "Code interpreter kickstarter"
future-titles: "Code interpreter kickstarter"
# It may be combined with a prompt more adept at continuing conversation
# Or a generic GPT-3 completion, actually.
# I need to format the input variables correctly so it remains an interperter
doc: "Given a line of code, infer the result of running that code"
prompt-version: 3
prompt: |+
    Code examples:

    # Language: Python
    print(random.randint(0,9))
    # Output: 5
    ###
    # Language: Bash
    Str="Learn Linux from LinuxHint"
    subStr=${Str:6:5}
    # Output: Linux
    ###
    # Language: <1>
    <2>
    # Output:
problems:
# prompt-filter: "sed -z 's/\s\+$//'"
# # Trailing whitespace is always removed
# prompt-remove-trailing-whitespace: on
# myrc will select the completion engine using my config.
# This may be openi-complete or something else
engine: "myrc"
# if nothing is selected in myrc and openapi-complete is used
# by default, then openai should select this engine.
preferred-openai-engine: "davinci"
# 0.0 = /r/hadastroke
# 1.0 = /r/iamveryrandom
# Use 0.3-0.8
temperature: 0.8
max-tokens: 60
top-p: 1.0
# Not available yet: openai api completions.create --help
frequency-penalty: 0.5
# If I make presence-penalty 0 then it will get very terse
presence-penalty: 0.0
best-of: 1
# Only the first one will be used by the API,
# but the completer script will use the others.
# Currently the API can only accept one stop-sequence, but that may change.
stop-sequences:
# 2 hashes is more reliable as a stop sequence because
# sometimes the generation morphs from ### to ##
- "##"
- "\n\n"
inject-start-text: yes
inject-restart-text: yes
show-probabilities: off
# Cache the function by default when running the prompt function
cache: on
vars:
- "language"
- "code"
examples:
- "haskell"
- "\"Hello\" ++ \" \" ++ \"World\""
# Completion is for generating a company-mode completion function
# completion: on
# # default values for pen -- evaled
# # This is useful for completion commands.
# pen-defaults:
# - "(detect-language)"
# - "(pen-preceding-text)"
# These are elisp String->String functions and run from pen.el
# It probably runs earlier than the preprocessors shell scripts
# pen-preprocessors:
# - "pen-pf-correct-code"
# # A preprocessor filters the var at that position
# the current implementation of preprocessors is kinda slow and will add ~100ml per variable
# # This may be useful to distinguish a block of text, for example
# preprocessors:
# - "sed 's/^/- /"
# - "cat"
chomp-start: on
chomp-end: off
prefer-external: on
# This is an optional external command which may be used to perform the same task as the API.
# This can be used to train the prompt.
# The external command must take arguments, not stdin
# So that all variables may be passed into it.
# iol haskell "[ x | x <- [50..100], x \`mod\` 7 == 3]"
external: "iol"
# This compares the output of the external script to the output of the LM
# $SCRIPTS/string-equal
similarity-test: "string-equal"
# This script returns a 0-1 decimal value representing the quality of the generated output.
# The input is 2 arguments each containing output
# The output is a decimal number from 0 to 1
quality-script: "levenshtein -s"
# This script can be used to validate the output.
# If the output is accurate, the validation script returns exit code 1.
# The input is 2 arguments each containing output
validation-script:
# Enable running conversation
conversation-mode: no
# Replace selected text
filter: no
# Keep stitching together until reaching this limit
# This allows a full response for answers which may need n*max-tokens to reach the stop-sequence.
stitch-max: 0
needs-work: no
n-test-runs: 5
# Prompt function aliases
# aliases:
# - "asktutor"
# Take the first line only
postprocessor: "sed 1q"
# postprocessor: "sed 's/- //' | uniqnosort"
# # Run it n times and combine the output
# n-collate: 10
# This for combining prompts:
# It might be, for example, summarize, or uniqnosort
# collation-postprocessor: "uniqnosort"