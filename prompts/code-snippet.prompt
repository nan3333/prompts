title: "code snippet from natural language"
aliases:
  - "code question"
  - "cq"
future-titles:
- "Get code snippet"
# Any language
- "Get snippet"
next-version: "get-code-snippet.prompt"
doc: "Get a bash one liner from natural langauge"
prompt: |+
    Language: Ruby
    Task: Reverse lines from stdout
    Code: while s = gets; puts s.chop.reverse; end
    Language: Haskell
    Task: Multiple Each Item in a List by 2
    Code:
    map (*2) [1..10]
    ###
    Language: bash
    Task: Recursively remove all "node_modules" folders
    Code:
    find . -name "node_modules" -exec rm -rf '{}' +
    ###
    Language: Haskell
    Task: Apply a function to both bits of a tuple:
    Code:
    both :: (a -> b) -> (a, a) -> (b, b)
    both = join (***)
    ###
    Language: Python
    Task: Palindrome Python One-Liner
    Code: phrase.find(phrase[::-1])
    ###
    Language: <1>    
    Task: <2>
    Code:
engine: "davinci"
# 0.0 = /r/hadastroke
# 1.0 = /r/iamveryrandom
# Use 0.3-0.8
temperature: 0.8
max-tokens: 60
top-p: 1.0
# Not available yet: openai api completions.create --help
frequency-penalty: 0.5
# If I make presence-penalty 0 then it will get very terse
presence-penalty: 0.0
best-of: 1
stop-sequences:
# - "\n"
# - "\n\n"
- "###"
inject-start-text: yes
inject-restart-text: yes
show-probabilities: off
vars:
- "language"
- "task"
examples:
- "bash"
- "suspend laptop"
chomp-start: on
chomp-end: off
external: ""
conversation-mode: no
filter: no
# Keep stitching together until reaching this limit
# This allows a full response for answers which may need n*max-tokens to reach the stop-sequence.
stitch-max: 0
needs-work: no